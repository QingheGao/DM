{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hernando/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import gc\n",
    "import pandas\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/training_set_VU_DM.csv',sep=',')\n",
    "# test = pd.read_csv('/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/test_set_VU_DM.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train.copy()\n",
    "# test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_process(df,file):\n",
    "    \n",
    "    df['avg_comp_rate'] = df[['comp1_rate', 'comp2_rate', 'comp3_rate', 'comp4_rate', 'comp5_rate', 'comp6_rate', 'comp7_rate', 'comp8_rate']].mean(axis=1)\n",
    "    df = df.drop(['comp1_rate', \"comp1_inv\", \"comp1_rate_percent_diff\", 'comp2_rate', \"comp2_inv\", \"comp2_rate_percent_diff\", 'comp3_rate', \"comp3_inv\", \"comp3_rate_percent_diff\", 'comp4_rate', \"comp4_inv\", \"comp4_rate_percent_diff\", 'comp5_rate', \"comp5_inv\", \"comp5_rate_percent_diff\", 'comp6_rate', \"comp6_inv\", \"comp6_rate_percent_diff\", 'comp7_rate', \"comp7_inv\", \"comp7_rate_percent_diff\", 'comp8_rate', \"comp8_inv\", \"comp8_rate_percent_diff\"], axis = 1)\n",
    "    df = df.fillna(value = {\"avg_comp_rate\": 0}) \n",
    "    \n",
    "    if file =='train':\n",
    "        df = df.drop(['gross_bookings_usd'],axis=1)\n",
    "\n",
    "#     df[\"orig_destination_distance\"].fillna(df[\"orig_destination_distance\"].describe()[6], inplace=True)\n",
    "#     df['visitor_hist_starrating'].fillna(df['visitor_hist_starrating'].median(axis = 0, skipna=True) ,inplace=True)\n",
    "#     df['visitor_hist_adr_usd'].fillna(df['visitor_hist_adr_usd'].median(axis = 0, skipna=True),inplace=True)\n",
    "#     df[\"srch_query_affinity_score\"].fillna(df[\"srch_query_affinity_score\"].min(), inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_change = missing_data_process(train_copy,file='train')\n",
    "# test_change = missing_data_process(test,file='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4958347, 30) (4959183, 27)\n"
     ]
    }
   ],
   "source": [
    "print(train_change.shape,test_change.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <td>94.920364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <td>94.897735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srch_query_affinity_score</th>\n",
       "      <td>93.598552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig_destination_distance</th>\n",
       "      <td>32.425766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_location_score2</th>\n",
       "      <td>21.990151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_review_score</th>\n",
       "      <td>0.148517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Missing Ratio\n",
       "visitor_hist_starrating        94.920364\n",
       "visitor_hist_adr_usd           94.897735\n",
       "srch_query_affinity_score      93.598552\n",
       "orig_destination_distance      32.425766\n",
       "prop_location_score2           21.990151\n",
       "prop_review_score               0.148517"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (train_change.isnull().sum() / len(train_change)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prop_location_score2</th>\n",
       "      <td>21.939743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_review_score</th>\n",
       "      <td>0.146516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Missing Ratio\n",
       "prop_location_score2      21.939743\n",
       "prop_review_score          0.146516"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (test_change.isnull().sum() / len(test_change)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_ten(input_,target):\n",
    "    input_data = input_.copy()\n",
    "    \n",
    "    input_data[target] = np.log10(input_data[target] + 1e-4)\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def normalize_combine(input_, group, target, combine = 'normal'):\n",
    "    input_data = input_.copy()\n",
    "    methods=['mean','std']\n",
    "    df = input_data.groupby(group).agg({target: methods})\n",
    "    ##drop the firs layer of column\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {}\n",
    "    for method in methods:\n",
    "        col[method] = target + \"_\" + method\n",
    "    df.rename(columns=col, inplace=True)\n",
    "    merge_data = input_data.merge(df.reset_index(), on=group)\n",
    "#     print(merge_data)\n",
    "    \n",
    "    ##using now price - mean /std\n",
    "    if combine =='normal':\n",
    "        merge_data[group + \"_norm_\" + target] = (merge_data[target] - merge_data[target + \"_mean\"]) /merge_data[target + \"_std\"]\n",
    "        merge_data = merge_data.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n",
    "    else:\n",
    "        merge_data['Mean_' + group + \"_norm_\" + target] = merge_data[target + \"_mean\"]\n",
    "        merge_data['Substract_' + group + \"_norm_\" + target] =(merge_data[target] - merge_data[target + \"_mean\"])\n",
    "        merge_data = merge_data.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n",
    "    gc.collect()\n",
    "    return merge_data\n",
    "\n",
    "# def data_processing(original_data, catogery = 'train'):\n",
    "#     print('Begin data processing>>>>>>>')\n",
    "#     gc.collect()\n",
    "#     data_training = original_data.copy()\n",
    "    \n",
    "#     if catogery == \"train\":\n",
    "#         target_score = [data_training[\"click_bool\"] == 1,data_training[\"booking_bool\"] == 1]\n",
    "#         choices = [1, 2]\n",
    "#         data_training['score'] = np.select(target_score, choices, default=0)\n",
    "#         score_final = data_training['score'].values\n",
    "#     else:\n",
    "#         score_final=1\n",
    "    \n",
    "#     ##processing date column\n",
    "#     dates = pandas.to_datetime(data_training[\"date_time\"])\n",
    "#     data_training[\"month\"] = dates.dt.month\n",
    "#     data_training[\"dayofweek\"] = dates.dt.dayofweek\n",
    "#     data_training[\"hour\"] = dates.dt.hour\n",
    "    \n",
    "#     print(\"Normalize training data>>>>>>\")\n",
    "#     data_training = log_ten(data_training,\"price_usd\")\n",
    "#     ## for price_use\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"price_usd\", combine='normal')\n",
    "#     data_training = normalize_combine(data_training,\"prop_id\",\"price_usd\", combine='normal')\n",
    "    \n",
    "#     ## for star\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_starrating\", combine='normal')\n",
    "# #     data_training = normalize_combine(data_training,\"prop_id\",\"prop_starrating\", combine='normal')\n",
    "    \n",
    "    \n",
    "#     print(\" Combine training data>>>>>>\")\n",
    "#     ##for pop \n",
    "#     data_training = normalize_combine(data_training,\"prop_id\",\"price_usd\",combine='combine')\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_starrating\",combine='combine')\n",
    "    \n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_location_score2\",combine='combine')\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_location_score1\",combine='combine')\n",
    "#     data_training = normalize_combine(data_training,\"srch_destination_id\",\"price_usd\",combine='combine')\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_review_score\",combine='combine')\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"promotion_flag\",combine='combine')\n",
    "\n",
    "#     print(\" Sort training data>>>>>>\")\n",
    "#     data_training = data_training.sort_values(\"srch_id\")\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_starrating\", combine='normal')\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_location_score2\",combine='normal')\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_location_score1\",combine='normal')\n",
    "#     data_training = normalize_combine(data_training,\"srch_id\",\"prop_review_score\", combine='normal')\n",
    "#     gc.collect()\n",
    "    \n",
    "#     #remove useless column\n",
    "#     if catogery == 'train':\n",
    "#         data_training = data_training.drop([\"click_bool\", \"booking_bool\",\"date_time\",'score'], axis=1)\n",
    "        \n",
    "\n",
    "#     return data_training,score_final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_normal(train_change):\n",
    "\n",
    "    train_10 = log_ten(train_change,\"price_usd\")\n",
    "    train_10 = normalize_combine(train_10,\"srch_id\",\"price_usd\", combine='normal')\n",
    "    train_10 = normalize_combine(train_10,\"prop_id\",\"price_usd\", combine='normal')\n",
    "    train_10 = normalize_combine(train_10,\"srch_id\",\"prop_starrating\", combine='normal')\n",
    "    train_10 = normalize_combine(train_10,\"prop_id\",\"price_usd\",combine='combine')\n",
    "    train_10 = normalize_combine(train_10,\"srch_id\",\"prop_starrating\",combine='combine')\n",
    "    train_10 = normalize_combine(train_10,\"srch_id\",\"prop_location_score2\",combine='combine')\n",
    "    train_10 = normalize_combine(train_10,\"srch_id\",\"prop_location_score1\",combine='combine')\n",
    "    train_10 = normalize_combine(train_10,\"srch_destination_id\",\"price_usd\",combine='combine')\n",
    "    train_10 = normalize_combine(train_10,\"srch_id\",\"prop_review_score\",combine='combine')\n",
    "    train_10 = normalize_combine(train_10,\"srch_id\",\"promotion_flag\",combine='combine')\n",
    "    train_30 = train_10.drop(['Substract_srch_destination_id_norm_price_usd'],axis=1)\n",
    "    train_30 = train_30.sort_values(\"srch_id\")\n",
    "    train_30 = normalize_combine(train_30,\"srch_id\",\"prop_starrating\", combine='normal')\n",
    "    train_30 = normalize_combine(train_30,\"srch_id\",\"prop_location_score2\",combine='normal')\n",
    "    train_30 = normalize_combine(train_30,\"srch_id\",\"prop_location_score1\",combine='normal')\n",
    "    train_30 = normalize_combine(train_30,\"srch_id\",\"prop_review_score\", combine='normal')\n",
    "\n",
    "    return train_30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_score = [train_30[\"click_bool\"] == 1,train_30[\"booking_bool\"] == 1]\n",
    "choices = [1, 2]\n",
    "train_30['score'] = np.select(target_score, choices, default=0)\n",
    "score_final = train_30['score'].values\n",
    "\n",
    "dates = pandas.to_datetime(train_30[\"date_time\"])\n",
    "train_30[\"month\"] = dates.dt.month\n",
    "train_30[\"dayofweek\"] = dates.dt.dayofweek\n",
    "train_30[\"hour\"] = dates.dt.hour\n",
    "\n",
    "train_30  = train_30.drop([\"click_bool\", \"booking_bool\",\"date_time\",'score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_30.to_csv('after_combine_missing.csv',index=False)\n",
    "# len(for_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_after_original = pd.read_csv('correct_answer.csv')\n",
    "# train_after_original = train_after_original.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_position(input_):\n",
    "    df = input_.copy()\n",
    "    position_dict = df.loc[df[\"random_bool\"] == 0]\n",
    "    ##main idea is using \"srch_destination_id\", \"prop_id\" to deal with position\n",
    "    \n",
    "    ##srch_destination_id:ID of the destination where the hotel search was performed\n",
    "    ## ID of hotel:The ID of the hotel\n",
    "    position_dict = df.groupby([\"srch_destination_id\", \"prop_id\"]).agg({\"position\": \"mean\"})\n",
    "    ##let the srch_destination_id prop_id become column\n",
    "    position_dict = position_dict.rename(index=str, columns={\"position\": \"es_position\"}).reset_index()\n",
    "    position_dict[\"srch_destination_id\"] = position_dict[\"srch_destination_id\"].astype(int)\n",
    "    position_dict[\"prop_id\"] = position_dict[\"prop_id\"].astype(int)\n",
    "    position_dict[\"es_position\"] = (1 / position_dict[\"es_position\"])\n",
    "    \n",
    "    return position_dict\n",
    "\n",
    "\n",
    "\n",
    "def pre_train_valid(input_,score):\n",
    "    df = input_.copy()\n",
    "    for_train = df[100000:]\n",
    "    for_valid = df[0:100000]\n",
    "    y_for_train = score[100000:]\n",
    "    y_for_valid = score[0:100000]\n",
    "    \n",
    "    #for prediction\n",
    "    print('Pre position>>>>>>>>.')\n",
    "    predict_dict = deal_position(df)\n",
    "\n",
    "    for_train = for_train.merge(predict_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"])\n",
    "    for_valid = for_valid.merge(predict_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"])\n",
    "    ###gruop number of srch_id -person\n",
    "    train_person = for_train[\"srch_id\"].value_counts(sort=False).sort_index()\n",
    "    vali_person = for_valid[\"srch_id\"].value_counts(sort=False).sort_index()\n",
    "    \n",
    "    for_train = for_train.drop([\"srch_id\",'position',\"prop_id\",\"random_bool\"],axis=1)\n",
    "    for_valid = for_valid.drop([\"srch_id\",'position',\"prop_id\",\"random_bool\"],axis=1)\n",
    "    \n",
    "    \n",
    "    return for_train, for_valid, y_for_train, y_for_valid, train_person, vali_person,predict_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre position>>>>>>>>.\n"
     ]
    }
   ],
   "source": [
    "for_train, for_valid, y_for_train, y_for_valid, train_person, vali_person,predict_dict = pre_train_valid(train_30, score_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model(for_train, for_valid, y_for_train, y_for_valid, train_person, vali_person,):\n",
    "    str(int(time.time()))\n",
    "    \n",
    "    features_cato = [\"site_id\",\"month\",\"prop_country_id\",\"visitor_location_country_id\",]\n",
    "    catogery_feature = [for_train.columns.get_loc(x) for x in features_cato]\n",
    "    print(catogery_feature)\n",
    "    \n",
    "    lgbm = lightgbm.LGBMRanker(n_estimators=500, learning_rate=0.12, max_position=5,label_gain=[0, 1, 2],\n",
    "                               random_state=69,\n",
    "                                seed=69,\n",
    "                                boosting=\"dart\",\n",
    "                              objective=\"lambdarank\",\n",
    "                                metric=\"ndcg\",)\n",
    "    lgbm.fit(for_train,y_for_train,eval_set=[(for_train,y_for_train), (for_valid, y_for_valid)],eval_group=[train_person, vali_person],group=train_person,eval_at=5,\n",
    "         verbose=20,\n",
    "        early_stopping_rounds=200,\n",
    "        categorical_feature= catogery_feature)\n",
    "    \n",
    "    return lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 42, 4, 1]\n",
      "[20]\ttraining's ndcg@5: 0.402322\tvalid_1's ndcg@5: 0.403171\n",
      "[40]\ttraining's ndcg@5: 0.405724\tvalid_1's ndcg@5: 0.406292\n",
      "[60]\ttraining's ndcg@5: 0.408709\tvalid_1's ndcg@5: 0.406257\n",
      "[80]\ttraining's ndcg@5: 0.409908\tvalid_1's ndcg@5: 0.408191\n",
      "[100]\ttraining's ndcg@5: 0.413648\tvalid_1's ndcg@5: 0.408588\n",
      "[120]\ttraining's ndcg@5: 0.417742\tvalid_1's ndcg@5: 0.410136\n",
      "[140]\ttraining's ndcg@5: 0.419257\tvalid_1's ndcg@5: 0.41238\n",
      "[160]\ttraining's ndcg@5: 0.420303\tvalid_1's ndcg@5: 0.413549\n",
      "[180]\ttraining's ndcg@5: 0.421618\tvalid_1's ndcg@5: 0.412952\n",
      "[200]\ttraining's ndcg@5: 0.422973\tvalid_1's ndcg@5: 0.413566\n",
      "[220]\ttraining's ndcg@5: 0.423844\tvalid_1's ndcg@5: 0.415128\n",
      "[240]\ttraining's ndcg@5: 0.426199\tvalid_1's ndcg@5: 0.415463\n",
      "[260]\ttraining's ndcg@5: 0.42733\tvalid_1's ndcg@5: 0.416175\n",
      "[280]\ttraining's ndcg@5: 0.428127\tvalid_1's ndcg@5: 0.415677\n",
      "[300]\ttraining's ndcg@5: 0.429603\tvalid_1's ndcg@5: 0.415492\n",
      "[320]\ttraining's ndcg@5: 0.431066\tvalid_1's ndcg@5: 0.416702\n",
      "[340]\ttraining's ndcg@5: 0.432821\tvalid_1's ndcg@5: 0.418074\n",
      "[360]\ttraining's ndcg@5: 0.434074\tvalid_1's ndcg@5: 0.416724\n",
      "[380]\ttraining's ndcg@5: 0.435557\tvalid_1's ndcg@5: 0.418724\n",
      "[400]\ttraining's ndcg@5: 0.43633\tvalid_1's ndcg@5: 0.418423\n",
      "[420]\ttraining's ndcg@5: 0.437791\tvalid_1's ndcg@5: 0.417738\n",
      "[440]\ttraining's ndcg@5: 0.439008\tvalid_1's ndcg@5: 0.419282\n",
      "[460]\ttraining's ndcg@5: 0.440898\tvalid_1's ndcg@5: 0.418942\n",
      "[480]\ttraining's ndcg@5: 0.442028\tvalid_1's ndcg@5: 0.418847\n",
      "[500]\ttraining's ndcg@5: 0.443116\tvalid_1's ndcg@5: 0.419808\n"
     ]
    }
   ],
   "source": [
    "lgbm =pre_model(for_train, for_valid, y_for_train, y_for_valid, train_person, vali_person,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/test_set_VU_DM.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_change = missing_data_process(test_copy,file='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_30 = combine_normal(test_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pandas.to_datetime(test_30[\"date_time\"])\n",
    "test_30[\"month\"] = dates.dt.month\n",
    "test_30[\"dayofweek\"] = dates.dt.dayofweek\n",
    "test_30[\"hour\"] = dates.dt.hour\n",
    "test_30  = test_30.drop([\"date_time\"], axis=1)\n",
    "for_test = test_30.merge(predict_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"])\n",
    "for_test = for_test.drop(['prop_id', 'random_bool', 'srch_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 42, 4, 1]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cato = [\"site_id\",\"month\",\"prop_country_id\",\"visitor_location_country_id\",]\n",
    "\n",
    "catogrey_for_test = [for_test.columns.get_loc(x) for x in features_cato]\n",
    "catogrey_for_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"categorical_feature\": catogrey_for_test}\n",
    "predictions = lgbm.predict(for_test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "testforfinal= test_copy[[\"srch_id\", \"prop_id\"]]\n",
    "testforfinal[\"prediction\"] = predictions\n",
    "testforfinal=testforfinal.sort_values( [\"srch_id\", \"prediction\"], ascending=False)\n",
    "testforfinal[[\"srch_id\", \"prop_id\"]].to_csv(\"submission_happy_change.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
