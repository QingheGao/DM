{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hernando/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import pandas\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/training_set_VU_DM.csv',sep=',')\n",
    "test = pd.read_csv('/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/test_set_VU_DM.csv',sep=',')\n",
    "# all_data = pd.concat((train,test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_with_missing_data(\n",
    "    df,\n",
    "    threshold=0.9,\n",
    "    ignore_values=[\n",
    "        \"visitor_hist_adr_usd\",\n",
    "        \"visitor_hist_starrating\",\n",
    "        \"srch_query_affinity_score\",\n",
    "    ],\n",
    "):\n",
    "    \n",
    "    df['avg_comp_rate'] = df[['comp1_rate', 'comp2_rate', 'comp3_rate', 'comp4_rate', 'comp5_rate', 'comp6_rate', 'comp7_rate', 'comp8_rate']].mean(axis=1)\n",
    "    df = df.drop(['comp1_rate', \"comp1_inv\", \"comp1_rate_percent_diff\", 'comp2_rate', \"comp2_inv\", \"comp2_rate_percent_diff\", 'comp3_rate', \"comp3_inv\", \"comp3_rate_percent_diff\", 'comp4_rate', \"comp4_inv\", \"comp4_rate_percent_diff\", 'comp5_rate', \"comp5_inv\", \"comp5_rate_percent_diff\", 'comp6_rate', \"comp6_inv\", \"comp6_rate_percent_diff\", 'comp7_rate', \"comp7_inv\", \"comp7_rate_percent_diff\", 'comp8_rate', \"comp8_inv\", \"comp8_rate_percent_diff\"], axis = 1)\n",
    "    df = df.fillna(value = {\"avg_comp_rate\": 0}) \n",
    "    columns_to_drop = []\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        length_df = len(df)\n",
    "        column_names = df.columns.tolist()\n",
    "        number_nans = sum(df.iloc[:, i].isnull())\n",
    "        if number_nans / length_df > threshold:\n",
    "            if column_names[i] not in ignore_values:\n",
    "                columns_to_drop.append(column_names[i])\n",
    "\n",
    "    print(\n",
    "        \"Dropping columns {} because they miss more than {} of data.\".format(\n",
    "            columns_to_drop, threshold\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df[\"orig_destination_distance\"].fillna(df[\"orig_destination_distance\"].describe()[6], inplace=True)\n",
    "    df['visitor_hist_starrating'].fillna(df['visitor_hist_starrating'].median(axis = 0, skipna=True) ,inplace=True)\n",
    "    df['visitor_hist_adr_usd'].fillna(df['visitor_hist_adr_usd'].median(axis = 0, skipna=True),inplace=True)\n",
    "    df[\"srch_query_affinity_score\"].fillna(df[\"srch_query_affinity_score\"].min(), inplace=True)\n",
    "    df['roomcount_bookwindow'] = df['srch_room_count']*max(df['srch_booking_window']) + df['srch_booking_window']\n",
    "    df['adultcount_childrencount'] = df['srch_adults_count']*max(df['srch_children_count']) + df['srch_children_count']\n",
    "\n",
    "    df_reduced = df.drop(labels=columns_to_drop, axis=1)\n",
    "    \n",
    "    \n",
    "    print(\"Dropped columns {}\".format(columns_to_drop))\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns ['gross_bookings_usd'] because they miss more than 0.9 of data.\n",
      "Dropped columns ['gross_bookings_usd']\n",
      "Dropping columns [] because they miss more than 0.9 of data.\n",
      "Dropped columns []\n"
     ]
    }
   ],
   "source": [
    "train_change = drop_columns_with_missing_data(train)\n",
    "test_change = drop_columns_with_missing_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_change.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4959183, 29)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_change.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prop_location_score2</th>\n",
       "      <td>21.990151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_review_score</th>\n",
       "      <td>0.148517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Missing Ratio\n",
       "prop_location_score2      21.990151\n",
       "prop_review_score          0.148517"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (train_change.isnull().sum() / len(train_change)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prop_location_score2</th>\n",
       "      <td>21.939743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_review_score</th>\n",
       "      <td>0.146516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Missing Ratio\n",
       "prop_location_score2      21.939743\n",
       "prop_review_score          0.146516"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (test_change.isnull().sum() / len(test_change)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(input_df, group_key, target_column, take_log10=False):\n",
    "    # for numerical stability\n",
    "    epsilon = 1e-8\n",
    "    if take_log10:\n",
    "        input_df[target_column] = np.log10(input_df[target_column] + epsilon)\n",
    "    methods = [\"mean\", \"std\"]\n",
    "\n",
    "    df = input_df.groupby(group_key).agg({target_column: methods})\n",
    "\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {}\n",
    "    for method in methods:\n",
    "        col[method] = target_column + \"_\" + method\n",
    "    \n",
    "    df.rename(columns=col, inplace=True)\n",
    "    df_merge = input_df.merge(df.reset_index(), on=group_key)\n",
    "    df_merge[target_column + \"_norm_by_\" + group_key] = (\n",
    "        df_merge[target_column] - df_merge[target_column + \"_mean\"]\n",
    "    ) / df_merge[target_column + \"_std\"]\n",
    "    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_merge\n",
    "\n",
    "\n",
    "\n",
    "def aggregated_features_single_column(\n",
    "    in_data,\n",
    "    key_for_grouped_by=\"prop_id\",\n",
    "    target_column=\"price_usd\",\n",
    "    agg_methods=[\"mean\", \"median\", \"min\", \"max\"],\n",
    "    transform_methods={\"mean\": [\"substract\"]},\n",
    "):\n",
    "    df = in_data.groupby(key_for_grouped_by).agg({target_column: agg_methods})\n",
    "\n",
    "    if isinstance(key_for_grouped_by, list):\n",
    "        str_key_for_grouped_by = \"|\".join(key_for_grouped_by)\n",
    "    else:\n",
    "        str_key_for_grouped_by = key_for_grouped_by\n",
    "\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {}\n",
    "    for method in agg_methods:\n",
    "        col[method] = (\n",
    "            method.upper() + \"(\" + str_key_for_grouped_by + \"_\" + target_column + \")\"\n",
    "        )\n",
    "\n",
    "    df.rename(columns=col, inplace=True)\n",
    "\n",
    "    in_data = in_data.merge(df.reset_index(), on=key_for_grouped_by)\n",
    "    for method_name in transform_methods:\n",
    "        for applying_function in transform_methods[method_name]:\n",
    "            function_data = in_data[\n",
    "                method_name.upper()\n",
    "                + \"(\"\n",
    "                + str_key_for_grouped_by\n",
    "                + \"_\"\n",
    "                + target_column\n",
    "                + \")\"\n",
    "            ]\n",
    "            column_data = in_data[target_column]\n",
    "            if applying_function == \"substract\":\n",
    "                result = column_data - function_data\n",
    "            elif applying_function == \"divide\":\n",
    "                result = column_data / function_data\n",
    "            else:\n",
    "                continue\n",
    "            in_data[\n",
    "                applying_function.upper()\n",
    "                + \"(\"\n",
    "                + target_column\n",
    "                + \"_\"\n",
    "                + method_name.upper()\n",
    "                + \")\"\n",
    "            ] = result\n",
    "    gc.collect()\n",
    "\n",
    "    return in_data\n",
    "\n",
    "\n",
    "\n",
    "def data_processing(orig_data, kind=\"train\", use_ndcg_choices=False):\n",
    "    \n",
    "    print(\"Preprocessing training data....\")\n",
    "    gc.collect()\n",
    "    data_training = orig_data\n",
    "    \n",
    "    target_column = \"target\"\n",
    "    \n",
    "    ##select the target column\n",
    "    if kind == \"train\":\n",
    "        conditions = [data_training[\"click_bool\"] == 1,data_training[\"booking_bool\"] == 1]\n",
    "        choices = [1, 2]\n",
    "        data_training[target_column] = np.select(conditions, choices, default=0)\n",
    "    \n",
    "    ## Add date column\n",
    "    dates = pandas.to_datetime(data_training[\"date_time\"])\n",
    "    data_training[\"month\"] = dates.dt.month\n",
    "    data_training[\"dayofweek\"] = dates.dt.dayofweek\n",
    "    data_training[\"hour\"] = dates.dt.hour\n",
    "#     data_training.drop(labels=[\"date_time\"], axis=1, inplace=True)\n",
    "    \n",
    "    ###normal\n",
    "    # do not normalize 2 times with take_log10\n",
    "    data_training = normalize_features(data_training, group_key=\"srch_id\",target_column=\"price_usd\",take_log10=True,)\n",
    "    \n",
    "    data_training = normalize_features(\n",
    "        data_training, group_key=\"prop_id\", target_column=\"price_usd\")\n",
    "    \n",
    "    data_training = normalize_features(\n",
    "        data_training, group_key=\"srch_id\", target_column=\"prop_starrating\")\n",
    "    \n",
    "    #### aggre new feature\n",
    "    data_training = aggregated_features_single_column(\n",
    "        data_training, \"prop_id\", \"price_usd\", [\"mean\"])\n",
    "    \n",
    "    \n",
    "    data_training = aggregated_features_single_column(\n",
    "        data_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"prop_starrating\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},)\n",
    "    \n",
    "    \n",
    "    data_training = aggregated_features_single_column(\n",
    "        data_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"prop_location_score2\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},)\n",
    "    \n",
    "    data_training = aggregated_features_single_column(\n",
    "        data_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"prop_location_score1\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},)\n",
    "    \n",
    "    data_training = aggregated_features_single_column(\n",
    "        data_training,\n",
    "        key_for_grouped_by=\"srch_destination_id\",\n",
    "        target_column=\"price_usd\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},)\n",
    "    \n",
    "    data_training = aggregated_features_single_column(\n",
    "        data_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"prop_review_score\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},)\n",
    "    \n",
    "    data_training = aggregated_features_single_column(\n",
    "        data_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"promotion_flag\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},)\n",
    "    # NOTE: has to be done after aggregated_features_single_column\n",
    "    data_training = data_training.sort_values(\"srch_id\")\n",
    "\n",
    "    data_training = normalize_features(\n",
    "        data_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n",
    "    )\n",
    "    data_training = normalize_features(\n",
    "        data_training, group_key=\"srch_id\", target_column=\"prop_location_score2\"\n",
    "    )\n",
    "    data_training = normalize_features(\n",
    "        data_training, group_key=\"srch_id\", target_column=\"prop_location_score1\"\n",
    "    )\n",
    "    data_training = normalize_features(\n",
    "        data_training, group_key=\"srch_id\", target_column=\"prop_review_score\"\n",
    "    )\n",
    "    gc.collect()\n",
    "    if kind == \"train\":\n",
    "        y = data_training[target_column].values\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    training_set_only_metrics = [\"click_bool\", \"booking_bool\", \"gross_bookings_usd\"]\n",
    "    columns_to_remove = [\n",
    "        \"date_time\",\n",
    "        \"target\",\n",
    "        target_column,\n",
    "    ] + training_set_only_metrics\n",
    "    columns_to_remove = [\n",
    "        c for c in columns_to_remove if c in data_training.columns.values\n",
    "    ]\n",
    "    data_training = data_training.drop(labels=columns_to_remove, axis=1)\n",
    "    return data_training, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data....\n"
     ]
    }
   ],
   "source": [
    "train_after = data_processing(train_change, kind=\"train\", use_ndcg_choices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(data_for_training, y, val_start=0, val_end=0):\n",
    "\n",
    "    x1 = pandas.concat([data_for_training[0:val_start], data_for_training[val_end:]])\n",
    "    y1 = np.concatenate((y[0:val_start], y[val_end:]), axis=0)\n",
    "    x2 = data_for_training[val_start:val_end]\n",
    "    y2 = y[val_start:val_end]\n",
    "\n",
    "    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n",
    "\n",
    "    # estimated position calculation\n",
    "    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n",
    "    srch_id_dest_id_dict = x1.groupby([\"srch_destination_id\", \"prop_id\"]).agg(\n",
    "        {\"position\": \"mean\"}\n",
    "    )\n",
    "    srch_id_dest_id_dict = srch_id_dest_id_dict.rename(\n",
    "        index=str, columns={\"position\": \"estimated_position\"}\n",
    "    ).reset_index()\n",
    "    srch_id_dest_id_dict[\"srch_destination_id\"] = (\n",
    "        srch_id_dest_id_dict[\"srch_destination_id\"].astype(str).astype(int)\n",
    "    )\n",
    "    srch_id_dest_id_dict[\"prop_id\"] = (\n",
    "        srch_id_dest_id_dict[\"prop_id\"].astype(str).astype(int)\n",
    "    )\n",
    "    srch_id_dest_id_dict[\"estimated_position\"] = (\n",
    "        1 / srch_id_dest_id_dict[\"estimated_position\"]\n",
    "    )\n",
    "    x1 = input_estimated_position(x1, srch_id_dest_id_dict)\n",
    "    x2 = input_estimated_position(x2, srch_id_dest_id_dict)\n",
    "\n",
    "    groups = x1[\"srch_id\"].value_counts(sort=False).sort_index()\n",
    "    eval_groups = x2[\"srch_id\"].value_counts(sort=False).sort_index()\n",
    "    len(eval_groups), len(x2), len(x1), len(groups)\n",
    "\n",
    "    x1 = remove_columns(x1)\n",
    "    x2 = remove_columns(x2)\n",
    "    return (x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict)\n",
    "def remove_columns(x1, ignore_column=[\"srch_id\", \"prop_id\", \"position\", \"random_bool\"]):\n",
    "    ignore_column = [c for c in ignore_column if c in x1.columns.values]\n",
    "    # print('Dropping columns: {}'.format(ignore_column))\n",
    "    # ignore_column_numbers = [x1.columns.get_loc(x) for x in ignore_column]\n",
    "    x1 = x1.drop(labels=ignore_column, axis=1)\n",
    "    # print('Columns after dropping: {}'.format(x1.columns.values))\n",
    "    return x1\n",
    "\n",
    "def input_estimated_position(training_data, srch_id_dest_id_dict):\n",
    "    training_data = training_data.merge(\n",
    "        srch_id_dest_id_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"]\n",
    "    )\n",
    "    print(training_data.head())\n",
    "    return training_data\n",
    "\n",
    "def train_model(\n",
    "    x1, x2, y1, y2, groups, eval_groups, lr, method, name_of_model=None\n",
    "):\n",
    "    if not name_of_model:\n",
    "        name_of_model = str(int(time.time()))\n",
    "\n",
    "    categorical_features_numbers = get_categorical_column(x1)\n",
    "#     df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n",
    "    print(categorical_features_numbers)\n",
    "    clf = lightgbm.LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        n_estimators=500,\n",
    "        learning_rate=lr,\n",
    "        max_position=5,\n",
    "        label_gain=[0, 1, 2],\n",
    "        random_state=69,\n",
    "        seed=69,\n",
    "        boosting=method,\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training on train set with columns: {}\".format(x1.columns.values))\n",
    "    clf.fit(\n",
    "        x1,\n",
    "        y1,\n",
    "        eval_set=[(x1, y1), (x2, y2)],\n",
    "        eval_group=[groups, eval_groups],\n",
    "        group=groups,\n",
    "        eval_at=5,\n",
    "        verbose=20,\n",
    "        early_stopping_rounds=200,\n",
    "        categorical_feature=categorical_features_numbers,\n",
    "    )\n",
    "#     gc.collect()\n",
    "#     pickle.dump(clf, open(os.path.join(output_dir, \"model.dat\"), \"wb\"))\n",
    "    return clf\n",
    "\n",
    "def get_categorical_column(x1):\n",
    "    categorical_features = [\n",
    "        \"day\",\n",
    "        \"month\",\n",
    "        \"prop_country_id\",\n",
    "        \"site_id\",\n",
    "        \"visitor_location_country_id\",\n",
    "    ]\n",
    "    categorical_features = [c for c in categorical_features if c in x1.columns.values]\n",
    "    categorical_features_numbers = [x1.columns.get_loc(x) for x in categorical_features]\n",
    "    return categorical_features_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-55883ac1e5d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mval_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalidation_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict = split_train_data(\n\u001b[0;32m---> 10\u001b[0;31m     train_after[0], train_after[1], val_start, val_end)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtest_after\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_change\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-1e024c664ffa>\u001b[0m in \u001b[0;36msplit_train_data\u001b[0;34m(data_for_training, y, val_start, val_end)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_for_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_for_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_start\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_for_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_start\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_for_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             )\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m             b = make_block(\n\u001b[0;32m-> 2022\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2023\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concatenating join units along axis0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_empty_dtype_and_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     to_concat = [\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m_get_empty_dtype_and_na\u001b[0;34m(join_units)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# are only null blocks, when same upcasting rules must be applied to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# null upcast classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_na\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mnull_upcast_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupcast_cls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mis_na\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mvalues_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalues_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mtotal_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mchunk_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "name_of_model = str(int(time.time()))\n",
    "# method = \"dart\"\n",
    "method = \"goss\"\n",
    "validation_num = 100000\n",
    "lr = 0.12\n",
    "# for i in range(0, int(len(training_data.index) / validation_num)): # enable for cross-validation\n",
    "val_start = 0 * validation_num\n",
    "val_end = 1 * validation_num\n",
    "x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict = split_train_data(\n",
    "    train_after[0], train_after[1], val_start, val_end)\n",
    "\n",
    "test_after, _ = data_processing(test_change, kind=\"test\")\n",
    "# predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir)\n",
    "print(\"Submit the predictions file submission.csv to kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 4, 0, 1]\n",
      "Training on train set with columns: ['site_id' 'visitor_location_country_id' 'visitor_hist_starrating'\n",
      " 'visitor_hist_adr_usd' 'prop_country_id' 'prop_starrating'\n",
      " 'prop_review_score' 'prop_brand_bool' 'prop_location_score1'\n",
      " 'prop_location_score2' 'prop_log_historical_price' 'price_usd'\n",
      " 'promotion_flag' 'srch_destination_id' 'srch_length_of_stay'\n",
      " 'srch_booking_window' 'srch_adults_count' 'srch_children_count'\n",
      " 'srch_room_count' 'srch_saturday_night_bool' 'srch_query_affinity_score'\n",
      " 'orig_destination_distance' 'avg_comp_rate' 'roomcount_bookwindow'\n",
      " 'adultcount_childrencount' 'month' 'dayofweek' 'hour'\n",
      " 'price_usd_norm_by_srch_id' 'price_usd_norm_by_prop_id'\n",
      " 'prop_starrating_norm_by_srch_id' 'MEAN(prop_id_price_usd)'\n",
      " 'SUBSTRACT(price_usd_MEAN)' 'MEAN(srch_id_prop_starrating)'\n",
      " 'SUBSTRACT(prop_starrating_MEAN)' 'MEAN(srch_id_prop_location_score2)'\n",
      " 'SUBSTRACT(prop_location_score2_MEAN)'\n",
      " 'MEAN(srch_id_prop_location_score1)'\n",
      " 'SUBSTRACT(prop_location_score1_MEAN)'\n",
      " 'MEAN(srch_destination_id_price_usd)' 'MEAN(srch_id_prop_review_score)'\n",
      " 'SUBSTRACT(prop_review_score_MEAN)' 'MEAN(srch_id_promotion_flag)'\n",
      " 'SUBSTRACT(promotion_flag_MEAN)' 'prop_location_score2_norm_by_srch_id'\n",
      " 'prop_location_score1_norm_by_srch_id'\n",
      " 'prop_review_score_norm_by_srch_id' 'estimated_position']\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[20]\ttraining's ndcg@5: 0.402261\tvalid_1's ndcg@5: 0.387282\n",
      "[40]\ttraining's ndcg@5: 0.411075\tvalid_1's ndcg@5: 0.39193\n",
      "[60]\ttraining's ndcg@5: 0.417505\tvalid_1's ndcg@5: 0.395262\n",
      "[80]\ttraining's ndcg@5: 0.422871\tvalid_1's ndcg@5: 0.398404\n",
      "[100]\ttraining's ndcg@5: 0.427158\tvalid_1's ndcg@5: 0.400021\n",
      "[120]\ttraining's ndcg@5: 0.430939\tvalid_1's ndcg@5: 0.402294\n",
      "[140]\ttraining's ndcg@5: 0.434202\tvalid_1's ndcg@5: 0.403209\n",
      "[160]\ttraining's ndcg@5: 0.436867\tvalid_1's ndcg@5: 0.40353\n",
      "[180]\ttraining's ndcg@5: 0.439479\tvalid_1's ndcg@5: 0.404165\n",
      "[200]\ttraining's ndcg@5: 0.441589\tvalid_1's ndcg@5: 0.406719\n",
      "[220]\ttraining's ndcg@5: 0.443865\tvalid_1's ndcg@5: 0.406537\n",
      "[240]\ttraining's ndcg@5: 0.446144\tvalid_1's ndcg@5: 0.406952\n",
      "[260]\ttraining's ndcg@5: 0.44801\tvalid_1's ndcg@5: 0.407878\n",
      "[280]\ttraining's ndcg@5: 0.449967\tvalid_1's ndcg@5: 0.407438\n",
      "[300]\ttraining's ndcg@5: 0.451788\tvalid_1's ndcg@5: 0.407002\n",
      "[320]\ttraining's ndcg@5: 0.453711\tvalid_1's ndcg@5: 0.407164\n",
      "[340]\ttraining's ndcg@5: 0.455523\tvalid_1's ndcg@5: 0.407689\n",
      "[360]\ttraining's ndcg@5: 0.45741\tvalid_1's ndcg@5: 0.408148\n",
      "[380]\ttraining's ndcg@5: 0.459329\tvalid_1's ndcg@5: 0.407968\n",
      "[400]\ttraining's ndcg@5: 0.461041\tvalid_1's ndcg@5: 0.407284\n",
      "[420]\ttraining's ndcg@5: 0.462817\tvalid_1's ndcg@5: 0.407327\n",
      "[440]\ttraining's ndcg@5: 0.464358\tvalid_1's ndcg@5: 0.408074\n",
      "[460]\ttraining's ndcg@5: 0.465938\tvalid_1's ndcg@5: 0.407513\n",
      "[480]\ttraining's ndcg@5: 0.467636\tvalid_1's ndcg@5: 0.408249\n",
      "[500]\ttraining's ndcg@5: 0.469419\tvalid_1's ndcg@5: 0.40842\n",
      "[520]\ttraining's ndcg@5: 0.471095\tvalid_1's ndcg@5: 0.409004\n",
      "[540]\ttraining's ndcg@5: 0.472745\tvalid_1's ndcg@5: 0.410031\n",
      "[560]\ttraining's ndcg@5: 0.474323\tvalid_1's ndcg@5: 0.409203\n",
      "[580]\ttraining's ndcg@5: 0.475675\tvalid_1's ndcg@5: 0.409353\n",
      "[600]\ttraining's ndcg@5: 0.477108\tvalid_1's ndcg@5: 0.409538\n",
      "[620]\ttraining's ndcg@5: 0.478571\tvalid_1's ndcg@5: 0.408962\n",
      "[640]\ttraining's ndcg@5: 0.480175\tvalid_1's ndcg@5: 0.408931\n",
      "[660]\ttraining's ndcg@5: 0.481867\tvalid_1's ndcg@5: 0.408767\n",
      "[680]\ttraining's ndcg@5: 0.483435\tvalid_1's ndcg@5: 0.408766\n",
      "[700]\ttraining's ndcg@5: 0.485043\tvalid_1's ndcg@5: 0.409445\n",
      "[720]\ttraining's ndcg@5: 0.486727\tvalid_1's ndcg@5: 0.408748\n",
      "[740]\ttraining's ndcg@5: 0.488319\tvalid_1's ndcg@5: 0.407247\n",
      "Early stopping, best iteration is:\n",
      "[547]\ttraining's ndcg@5: 0.473334\tvalid_1's ndcg@5: 0.410061\n"
     ]
    }
   ],
   "source": [
    "model = train_model(\n",
    "    x1, x2, y1, y2, groups, eval_groups, lr, method, name_of_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data, srch_id_dest_id_dict):\n",
    "\n",
    "    test_data = test_data.copy()\n",
    "    test_data = input_estimated_position(test_data, srch_id_dest_id_dict)\n",
    "\n",
    "    test_data_srch_id_prop_id = test_data[[\"srch_id\", \"prop_id\"]]\n",
    "\n",
    "    test_data = remove_columns(test_data)\n",
    "\n",
    "    categorical_features_numbers = get_categorical_column(test_data)\n",
    "\n",
    "    print(\"Predicting on train set with columns: {}\".format(test_data.columns.values))\n",
    "    kwargs = {}\n",
    "    kwargs = {\"categorical_feature\": categorical_features_numbers}\n",
    "\n",
    "    predictions = model.predict(test_data, **kwargs)\n",
    "    \n",
    "    test_data_srch_id_prop_id[\"prediction\"] = predictions\n",
    "    del test_data\n",
    "    gc.collect()\n",
    "\n",
    "    test_data_srch_id_prop_id = test_data_srch_id_prop_id.sort_values(\n",
    "        [\"srch_id\", \"prediction\"], ascending=False\n",
    "    )\n",
    "    print(\"Saving predictions into submission.csv\")\n",
    "    test_data_srch_id_prop_id[[\"srch_id\", \"prop_id\"]].to_csv(\n",
    "         \"submission.csv\", index=False\n",
    "    )\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   srch_id  site_id  visitor_location_country_id  visitor_hist_starrating  \\\n",
      "0        1       24                          216                     3.43   \n",
      "1        1       24                          216                     3.43   \n",
      "2        1       24                          216                     3.43   \n",
      "3        1       24                          216                     3.43   \n",
      "4        1       24                          216                     3.43   \n",
      "\n",
      "   visitor_hist_adr_usd  prop_country_id  prop_id  prop_starrating  \\\n",
      "0                151.39              219     3180                3   \n",
      "1                151.39              219   139162                3   \n",
      "2                151.39              219   134992                2   \n",
      "3                151.39              219   128871                2   \n",
      "4                151.39              219   128085                2   \n",
      "\n",
      "   prop_review_score  prop_brand_bool  ...  \\\n",
      "0                4.5                1  ...   \n",
      "1                4.5                1  ...   \n",
      "2                5.0                1  ...   \n",
      "3                3.0                1  ...   \n",
      "4                3.0                1  ...   \n",
      "\n",
      "   SUBSTRACT(prop_location_score1_MEAN)  MEAN(srch_destination_id_price_usd)  \\\n",
      "0                               0.27931                             1.923635   \n",
      "1                              -0.46069                             1.923635   \n",
      "2                               0.37931                             1.923635   \n",
      "3                              -0.10069                             1.923635   \n",
      "4                              -0.26069                             1.923635   \n",
      "\n",
      "   MEAN(srch_id_prop_review_score)  SUBSTRACT(prop_review_score_MEAN)  \\\n",
      "0                         4.017241                           0.482759   \n",
      "1                         4.017241                           0.482759   \n",
      "2                         4.017241                           0.982759   \n",
      "3                         4.017241                          -1.017241   \n",
      "4                         4.017241                          -1.017241   \n",
      "\n",
      "   MEAN(srch_id_promotion_flag)  SUBSTRACT(promotion_flag_MEAN)  \\\n",
      "0                      0.206897                       -0.206897   \n",
      "1                      0.206897                       -0.206897   \n",
      "2                      0.206897                       -0.206897   \n",
      "3                      0.206897                       -0.206897   \n",
      "4                      0.206897                        0.793103   \n",
      "\n",
      "   prop_location_score2_norm_by_srch_id  prop_location_score1_norm_by_srch_id  \\\n",
      "0                             -0.734785                              0.928059   \n",
      "1                             -0.830088                             -1.530724   \n",
      "2                              0.192137                              1.260327   \n",
      "3                             -1.024611                             -0.334559   \n",
      "4                             -0.960640                             -0.866188   \n",
      "\n",
      "   prop_review_score_norm_by_srch_id  estimated_position  \n",
      "0                           0.513567            0.066815  \n",
      "1                           0.513567            0.057042  \n",
      "2                           1.045476            0.036395  \n",
      "3                          -1.082159            0.034454  \n",
      "4                          -1.082159            0.071546  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "Predicting on train set with columns: ['site_id' 'visitor_location_country_id' 'visitor_hist_starrating'\n",
      " 'visitor_hist_adr_usd' 'prop_country_id' 'prop_starrating'\n",
      " 'prop_review_score' 'prop_brand_bool' 'prop_location_score1'\n",
      " 'prop_location_score2' 'prop_log_historical_price' 'price_usd'\n",
      " 'promotion_flag' 'srch_destination_id' 'srch_length_of_stay'\n",
      " 'srch_booking_window' 'srch_adults_count' 'srch_children_count'\n",
      " 'srch_room_count' 'srch_saturday_night_bool' 'srch_query_affinity_score'\n",
      " 'orig_destination_distance' 'avg_comp_rate' 'roomcount_bookwindow'\n",
      " 'adultcount_childrencount' 'month' 'dayofweek' 'hour'\n",
      " 'price_usd_norm_by_srch_id' 'price_usd_norm_by_prop_id'\n",
      " 'prop_starrating_norm_by_srch_id' 'MEAN(prop_id_price_usd)'\n",
      " 'SUBSTRACT(price_usd_MEAN)' 'MEAN(srch_id_prop_starrating)'\n",
      " 'SUBSTRACT(prop_starrating_MEAN)' 'MEAN(srch_id_prop_location_score2)'\n",
      " 'SUBSTRACT(prop_location_score2_MEAN)'\n",
      " 'MEAN(srch_id_prop_location_score1)'\n",
      " 'SUBSTRACT(prop_location_score1_MEAN)'\n",
      " 'MEAN(srch_destination_id_price_usd)' 'MEAN(srch_id_prop_review_score)'\n",
      " 'SUBSTRACT(prop_review_score_MEAN)' 'MEAN(srch_id_promotion_flag)'\n",
      " 'SUBSTRACT(promotion_flag_MEAN)' 'prop_location_score2_norm_by_srch_id'\n",
      " 'prop_location_score1_norm_by_srch_id'\n",
      " 'prop_review_score_norm_by_srch_id' 'estimated_position']\n",
      "Saving predictions into submission.csv\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(model, test_after, srch_id_dest_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <td>94.927760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <td>94.905057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srch_query_affinity_score</th>\n",
       "      <td>93.596999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp2_rate_percent_diff</th>\n",
       "      <td>88.780134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <td>87.598498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp5_rate_percent_diff</th>\n",
       "      <td>83.032645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp3_rate</th>\n",
       "      <td>69.051408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp3_inv</th>\n",
       "      <td>66.699188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp8_rate</th>\n",
       "      <td>61.335944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp8_inv</th>\n",
       "      <td>59.907783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp2_rate</th>\n",
       "      <td>59.159978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp2_inv</th>\n",
       "      <td>57.028985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp5_rate</th>\n",
       "      <td>55.186136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp5_inv</th>\n",
       "      <td>52.409534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig_destination_distance</th>\n",
       "      <td>32.432636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_location_score2_norm_by_srch_id</th>\n",
       "      <td>22.107190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBSTRACT(prop_location_score2_MEAN)</th>\n",
       "      <td>21.995588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_location_score2</th>\n",
       "      <td>21.995588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN(srch_id_prop_location_score2)</th>\n",
       "      <td>1.856846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_starrating_norm_by_srch_id</th>\n",
       "      <td>1.820990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_usd_norm_by_prop_id</th>\n",
       "      <td>0.315539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_review_score_norm_by_srch_id</th>\n",
       "      <td>0.195210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBSTRACT(prop_review_score_MEAN)</th>\n",
       "      <td>0.148672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_review_score</th>\n",
       "      <td>0.148672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_location_score1_norm_by_srch_id</th>\n",
       "      <td>0.021077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN(srch_id_prop_review_score)</th>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Missing Ratio\n",
       "visitor_hist_starrating                   94.927760\n",
       "visitor_hist_adr_usd                      94.905057\n",
       "srch_query_affinity_score                 93.596999\n",
       "comp2_rate_percent_diff                   88.780134\n",
       "comp8_rate_percent_diff                   87.598498\n",
       "comp5_rate_percent_diff                   83.032645\n",
       "comp3_rate                                69.051408\n",
       "comp3_inv                                 66.699188\n",
       "comp8_rate                                61.335944\n",
       "comp8_inv                                 59.907783\n",
       "comp2_rate                                59.159978\n",
       "comp2_inv                                 57.028985\n",
       "comp5_rate                                55.186136\n",
       "comp5_inv                                 52.409534\n",
       "orig_destination_distance                 32.432636\n",
       "prop_location_score2_norm_by_srch_id      22.107190\n",
       "SUBSTRACT(prop_location_score2_MEAN)      21.995588\n",
       "prop_location_score2                      21.995588\n",
       "MEAN(srch_id_prop_location_score2)         1.856846\n",
       "prop_starrating_norm_by_srch_id            1.820990\n",
       "price_usd_norm_by_prop_id                  0.315539\n",
       "prop_review_score_norm_by_srch_id          0.195210\n",
       "SUBSTRACT(prop_review_score_MEAN)          0.148672\n",
       "prop_review_score                          0.148672\n",
       "prop_location_score1_norm_by_srch_id       0.021077\n",
       "MEAN(srch_id_prop_review_score)            0.000206"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (x1.isnull().sum() / len(x1)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(all_data):\n",
    "    all_data[\"orig_destination_distance\"].fillna(all_data[\"orig_destination_distance\"].describe()[6], inplace=True)\n",
    "    ####this step can change\n",
    "#     all_data['prop_review_score'].fillna(3, inplace=True)\n",
    "#     all_data['prop_review_score'][all_data['prop_review_score']==0]=2.5\n",
    "    all_data['prop_review_score'].fillna(0, inplace=True)\n",
    "\n",
    "    ##\n",
    "#     all_data['prop_location_score2'].fillna(0, inplace=True)\n",
    "    all_data['prop_location_score2'].fillna(all_data[\"prop_location_score2\"].median(axis = 0, skipna=True), inplace=True)\n",
    "\n",
    "    ##\n",
    "    #replace NaN for worst case scenario, in this case -326.567500 which is the minimum value for this feature\n",
    "    all_data[\"srch_query_affinity_score\"].fillna(all_data[\"srch_query_affinity_score\"].min(), inplace=True)\n",
    "\n",
    "    ##\n",
    "#     all_data['visitor_hist_starrating'].fillna(0 ,inplace=True)\n",
    "#     all_data['visitor_hist_adr_usd'].fillna(0,inplace=True)\n",
    "\n",
    "    all_data['visitor_hist_starrating'].fillna(all_data['visitor_hist_starrating'].median(axis = 0, skipna=True) ,inplace=True)\n",
    "    all_data['visitor_hist_adr_usd'].fillna(all_data['visitor_hist_adr_usd'].median(axis = 0, skipna=True),inplace=True)\n",
    "\n",
    "\n",
    "    #removing outliers\n",
    "    # indices_to_remove = all_data[all_data['price_usd'] > 50000].index.tolist()\n",
    "    # all_data = all_data.drop(indices_to_remove)\n",
    "\n",
    "    indices_to_remove = all_data[all_data['price_usd'] > train['price_usd'][train['booking_bool']==1].max()].index.tolist()\n",
    "    all_data = all_data.drop(indices_to_remove)\n",
    "    return all_data\n",
    "#开头0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_na(all_data):\n",
    "#     all_data[\"orig_destination_distance\"].fillna(all_data[\"orig_destination_distance\"].describe()[6], inplace=True)\n",
    "#     ####this step can change\n",
    "# #     all_data['prop_review_score'].fillna(3, inplace=True)\n",
    "# #     all_data['prop_review_score'][all_data['prop_review_score']==0]=2.5\n",
    "#     all_data['prop_review_score'].fillna(0, inplace=True)\n",
    "\n",
    "#     ##\n",
    "#     all_data['prop_location_score2'].fillna(0, inplace=True)\n",
    "# #     all_data['prop_location_score2'].fillna(all_data[\"prop_location_score2\"].median(axis = 0, skipna=True), inplace=True)\n",
    "\n",
    "#     ##\n",
    "#     #replace NaN for worst case scenario, in this case -326.567500 which is the minimum value for this feature\n",
    "#     all_data[\"srch_query_affinity_score\"].fillna(all_data[\"srch_query_affinity_score\"].min(), inplace=True)\n",
    "\n",
    "#     ##\n",
    "# #     all_data['visitor_hist_starrating'].fillna(0 ,inplace=True)\n",
    "# #     all_data['visitor_hist_adr_usd'].fillna(0,inplace=True)\n",
    "\n",
    "#     all_data['visitor_hist_starrating'].fillna(all_data['visitor_hist_starrating'].median(axis = 0, skipna=True) ,inplace=True)\n",
    "#     all_data['visitor_hist_adr_usd'].fillna(all_data['visitor_hist_adr_usd'].median(axis = 0, skipna=True),inplace=True)\n",
    "\n",
    "\n",
    "#     #removing outliers\n",
    "#     # indices_to_remove = all_data[all_data['price_usd'] > 50000].index.tolist()\n",
    "#     # all_data = all_data.drop(indices_to_remove)\n",
    "\n",
    "#     indices_to_remove = all_data[all_data['price_usd'] > train['price_usd'][train['booking_bool']==1].max()].index.tolist()\n",
    "#     all_data = all_data.drop(indices_to_remove)\n",
    "#     return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature - feature combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotelqualit(dataset):\n",
    "    hotel_quality = pd.DataFrame(dataset.prop_id.value_counts(dropna = False))\n",
    "    hotel_quality = hotel_quality.join(pd.DataFrame(dataset.prop_id[dataset.booking_bool == 1].value_counts().astype(int)), rsuffix = \"book\")\n",
    "    hotel_quality = hotel_quality.join(pd.DataFrame(dataset.prop_id[dataset.click_bool == 1].value_counts().astype(int)), rsuffix = \"click\")\n",
    "    hotel_quality.columns = [\"counts\", \"booked\", \"clicked\"]\n",
    "\n",
    "    hotel_quality[\"booked_percentage\"] = hotel_quality.booked / hotel_quality.counts * 100\n",
    "    hotel_quality[\"clicked_percentage\"] = hotel_quality.clicked / hotel_quality.counts * 100\n",
    "    return hotel_quality\n",
    "\n",
    "def f_com(dataset,train_set):\n",
    "    data=dataset\n",
    "    data[\"starrating_diff\"] = abs(data[\"visitor_hist_starrating\"] - data[\"prop_starrating\"])\n",
    "    data[\"usd_diff\"] = abs(np.log10(data[\"visitor_hist_adr_usd\"]) - np.log10(data[\"price_usd\"]))\n",
    "    \n",
    "    data= data.replace([np.inf, -np.inf], np.nan)\n",
    "    data=data.fillna(value={\"usd_diff\":1.1})\n",
    "    \n",
    "#     data = data.fillna(value = {\"starrating_diff\": 6, \"usd_diff\": 1.1})\n",
    "    #difference between stars and review stars\n",
    "    data['ad_vs_real'] = data['prop_starrating'] - data['prop_review_score']\n",
    "#     data=data.drop([\"visitor_hist_starrating\"], axis=1)\n",
    "#     data['prop_starrating_monotonic'] =  abs(data['prop_starrating']- data['prop_starrating'][data['booking_bool']==1].mean())\n",
    "    \n",
    "    hotel_quality = hotelqualit(train_set)\n",
    "    \n",
    "    data = data.join(hotel_quality.booked_percentage, on = \"prop_id\")\n",
    "    data = data.join(hotel_quality.clicked_percentage, on = \"prop_id\")\n",
    "    data.fillna(value = {\"booked_percentage\": data[\"booked_percentage\"].mean(), \"clicked_percentage\": data[\"clicked_percentage\"].mean()},inplace=True)\n",
    "    \n",
    "    \n",
    "####own method\n",
    "#     for i in range(1,9):\n",
    "#         data['comp'+str(i)+'_inv'].fillna(0, inplace=True)\n",
    "#         data['comp'+str(i)+'_inv'][data['comp'+str(i)+'_inv']==1] = 10\n",
    "#         data['comp'+str(i)+'_inv'][data['comp'+str(i)+'_inv']==-1] = 1\n",
    "#         data['comp'+str(i)+'_inv'][data['comp'+str(i)+'_inv']==0] = -1\n",
    "#         data['comp'+str(i)+'_inv'][data['comp'+str(i)+'_inv']==10] = 0\n",
    "#     data['comp_inv_sum'] = data['comp1_inv']\n",
    "#     for i in range(2,9):\n",
    "#         data['comp_inv_sum'] += data['comp'+str(i)+'_inv']\n",
    "        \n",
    "#     for i in range(1,9):\n",
    "#         data['comp'+str(i)+'_rate'].fillna(0, inplace=True)\n",
    "#         data['comp'+str(i)+'_rate_percent_diff'].fillna(0, inplace=True)\n",
    "#     data['comp_rate_sum'] = data['comp1_rate']\n",
    "#     for i in range(2,9):\n",
    "#         data['comp_rate_sum'] += data['comp'+str(i)+'_rate']\n",
    "        \n",
    "#     data['comp_sum'] = data['comp1_rate'] + data['comp1_inv'] + 2\n",
    "#     for i in range(2,9):\n",
    "#         data['comp_sum'] += data['comp'+str(i)+'_rate'] + data['comp'+str(i)+'_inv']\n",
    "        \n",
    "###\n",
    "    # Average comp price\n",
    "    data['avg_comp_rate'] = data[['comp1_rate', 'comp2_rate', 'comp3_rate', 'comp4_rate', 'comp5_rate', 'comp6_rate', 'comp7_rate', 'comp8_rate']].mean(axis=1)\n",
    "    data = data.drop(['comp1_rate', \"comp1_inv\", \"comp1_rate_percent_diff\", 'comp2_rate', \"comp2_inv\", \"comp2_rate_percent_diff\", 'comp3_rate', \"comp3_inv\", \"comp3_rate_percent_diff\", 'comp4_rate', \"comp4_inv\", \"comp4_rate_percent_diff\", 'comp5_rate', \"comp5_inv\", \"comp5_rate_percent_diff\", 'comp6_rate', \"comp6_inv\", \"comp6_rate_percent_diff\", 'comp7_rate', \"comp7_inv\", \"comp7_rate_percent_diff\", 'comp8_rate', \"comp8_inv\", \"comp8_rate_percent_diff\"], axis = 1)\n",
    "    data = data.fillna(value = {\"avg_comp_rate\": 0}) \n",
    "\n",
    "### DATE\n",
    "    data['date_time']= pd.to_datetime(data[\"date_time\"],format='%Y-%m-%d %H:%M:%S')\n",
    "    data[\"year\"]= data[\"date_time\"].dt.year\n",
    "    data[\"month\"] = data[\"date_time\"].dt.month\n",
    "    data[\"day\"] = data[\"date_time\"].dt.day\n",
    "#     data = data.drop(['date_time'],axis=1)\n",
    "    \n",
    "    \n",
    "### new feature\n",
    "    #Compose features roomcount_bookwindow, adultcount_childrencount by F1_F2 = F1*max(Max(F2)) + F2\n",
    "    data['roomcount_bookwindow'] = data['srch_room_count']*max(data['srch_booking_window']) + data['srch_booking_window']\n",
    "    data['adultcount_childrencount'] = data['srch_adults_count']*max(data['srch_children_count']) + data['srch_children_count']\n",
    "#     data=data.drop([\"srch_room_count\",\"srch_children_count\"], axis=1)\n",
    "    \n",
    "    #1 if hotel is in the same country\n",
    "    data['bool_same_country'] = 0\n",
    "    data.loc[(data['visitor_location_country_id'] == data['prop_country_id']), 'bool_same_country'] = 1\n",
    "    #there are some infinite values in usd_diff\n",
    "#     data= data.replace([np.inf, -np.inf], np.nan)\n",
    "    #train.columns[train.isna().any()].tolist()\n",
    "#     data=data.fillna(value={\"usd_diff\":1.1})\n",
    "    \n",
    "#     data=data.drop([\"random_bool\" ,\"site_id\",'srch_saturday_night_bool',\"srch_query_affinity_score\"], axis=1)\n",
    "   \n",
    "    return data\n",
    "\n",
    "def drop_feature(data):\n",
    "#     data=data.drop([\"visitor_hist_starrating\"], axis=1)\n",
    "    data = data.drop(['date_time','year','day'],axis=1)\n",
    "#     data=data.drop([\"srch_room_count\",\"srch_children_count\"], axis=1)\n",
    "    data=data.drop([\"site_id\"], axis=1)\n",
    "    data=data.drop(['ad_vs_real','roomcount_bookwindow','adultcount_childrencount','bool_same_country','orig_destination_distance'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_change = fill_na(train)\n",
    "test_change = fill_na(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_change = f_com(test_change,train_change)\n",
    "train_change = f_com(train_change,train_change)\n",
    "test_change =  drop_feature(test_change)\n",
    "train_change = drop_feature(train_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_change.to_csv(\"test_after_feature.csv\", index=False)\n",
    "train_change.to_csv(\"train_after_feature.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_score(x):\n",
    "    if x[\"booking_bool\"]==1:\n",
    "        val=5\n",
    "    elif x[\"click_bool\"]==1:\n",
    "        val=1\n",
    "    else:\n",
    "        val=0\n",
    "    return val\n",
    "\n",
    "def assign_newscore(x):\n",
    "    pos= x[\"position\"]\n",
    "    if pos==1:\n",
    "        val=5\n",
    "    elif pos ==2:\n",
    "        val=4\n",
    "    elif pos >= 3 and pos <= 5:\n",
    "        val=3\n",
    "    elif pos >=6 and pos <=10:\n",
    "        val=2\n",
    "    else:\n",
    "        val=1\n",
    "    return val\n",
    "    \n",
    "    \n",
    "\n",
    "train_change['score_book'] = train_change.apply(assign_score , axis=1)\n",
    "train_change['score_position'] = train_change.apply(assign_newscore , axis=1)\n",
    "train_change['score']= train_change['score_book']+train_change['score_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_data = pd.DataFrame(columns=['Score'])\n",
    "score_data['Score'] = train_change['score']\n",
    "score_data.to_csv(\"score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
