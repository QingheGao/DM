{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <td>97.208943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Missing Ratio\n",
       "gross_bookings_usd      97.208943"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (train_change.isnull().sum() / len(train_change)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Ratio]\n",
       "Index: []"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (test_change.isnull().sum() / len(test_change)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_result = pd.read_csv('score.csv')\n",
    "train_change = pd.read_csv('/Users/hernando/Desktop/datamining1/assign2/code/train_after_feature.csv')\n",
    "test_change = pd.read_csv('/Users/hernando/Desktop/datamining1/assign2/code/test_after_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_id_train=train_change[\"srch_id\"]\n",
    "train_final=train_change.drop([\"srch_id\",'prop_id',\"position\",'click_bool','booking_bool','gross_bookings_usd'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Ratio]\n",
       "Index: []"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (train_final.isnull().sum() / len(train_final)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958336, 27)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visitor_location_country_id',\n",
       " 'visitor_hist_starrating',\n",
       " 'visitor_hist_adr_usd',\n",
       " 'prop_country_id',\n",
       " 'prop_starrating',\n",
       " 'prop_review_score',\n",
       " 'prop_brand_bool',\n",
       " 'prop_location_score1',\n",
       " 'prop_location_score2',\n",
       " 'prop_log_historical_price',\n",
       " 'price_usd',\n",
       " 'promotion_flag',\n",
       " 'srch_destination_id',\n",
       " 'srch_length_of_stay',\n",
       " 'srch_booking_window',\n",
       " 'srch_adults_count',\n",
       " 'srch_children_count',\n",
       " 'srch_room_count',\n",
       " 'srch_saturday_night_bool',\n",
       " 'srch_query_affinity_score',\n",
       " 'random_bool',\n",
       " 'starrating_diff',\n",
       " 'usd_diff',\n",
       " 'booked_percentage',\n",
       " 'clicked_percentage',\n",
       " 'avg_comp_rate',\n",
       " 'month']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in train_final.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In conclusion PCA is bullshit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "y=score_result[\"Score\"]\n",
    "X=train_final\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=13 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.475526909473559\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr = RandomForestRegressor(n_jobs=-1)\n",
    "rfr = regr.fit(X_train, y_train)\n",
    "prediction = rfr.predict(X_test)\n",
    "print(r2_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r^2 on test data : 0.120142\n"
     ]
    }
   ],
   "source": [
    "regr = ElasticNet(random_state=0, alpha=1e-05,l1_ratio=0.2)\n",
    "trained_model_en=regr.fit(X_train, y_train)\n",
    "y_pred_enet = trained_model_en.predict(X_test)\n",
    "r2_score_enet = r2_score(y_test, y_pred_enet)\n",
    "print(\"r^2 on test data : %f\" % r2_score_enet)\n",
    "#0.054626\n",
    "#0.120514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4959165, 27)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_id_test=test_change[\"srch_id\"]\n",
    "new_test= test_change.drop([\"srch_id\",'prop_id'], axis=1)\n",
    "new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction= trained_model_en.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.56470143, 1.7595035 , 1.21922794, ..., 1.69006398, 2.08863397,\n",
       "       2.05226817])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorting the ranking...\n",
      "\n",
      "\n",
      "Store the predictions...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(test_prediction)\n",
    "submiss_test_set = pd.DataFrame(test_change[\"srch_id\"])\n",
    "submiss_test_set.columns = [\"srch_id\"]\n",
    "submiss_test_set[\"ranking\"] = predictions_df\n",
    "submiss_test_set[\"prop_id\"] = test_change[\"prop_id\"]\n",
    "# Group by search id and sort by ranking!\n",
    "print(\"\\nSorting the ranking...\\n\")\n",
    "test_set_submission_result = submiss_test_set.groupby([\"srch_id\"]).apply(\n",
    "    lambda x: x.sort_values([\"ranking\"], ascending=False)).reset_index(drop=True)\n",
    "print(\"\\nStore the predictions...\\n\")\n",
    "test_set_submission_result = test_set_submission_result.drop(\"ranking\", axis=1)\n",
    "# store the file to submit!\n",
    "test_set_submission_result.to_csv(\"RESULT_to_submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isfinite(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try average+stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0011, random_state=1))\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.1,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=11, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r^2 on test data : 0.195356\n"
     ]
    }
   ],
   "source": [
    "# score = rmsle_cv(GBoost) # lightgbm xgboost\n",
    "# print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "trained_model_en=GBoost.fit(X_train, y_train)\n",
    "y_pred_enet = trained_model_en.predict(X_test)\n",
    "r2_score_enet = r2_score(y_test, y_pred_enet)\n",
    "print(\"r^2 on test data : %f\" % r2_score_enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction= trained_model_en.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorting the ranking...\n",
      "\n",
      "\n",
      "Store the predictions...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(test_prediction)\n",
    "submiss_test_set = pd.DataFrame(test_change[\"srch_id\"])\n",
    "submiss_test_set.columns = [\"srch_id\"]\n",
    "submiss_test_set[\"ranking\"] = predictions_df\n",
    "submiss_test_set[\"prop_id\"] = test_change[\"prop_id\"]\n",
    "# Group by search id and sort by ranking!\n",
    "print(\"\\nSorting the ranking...\\n\")\n",
    "test_set_submission_result = submiss_test_set.groupby([\"srch_id\"]).apply(\n",
    "    lambda x: x.sort_values([\"ranking\"], ascending=False)).reset_index(drop=True)\n",
    "print(\"\\nStore the predictions...\\n\")\n",
    "test_set_submission_result = test_set_submission_result.drop(\"ranking\", axis=1)\n",
    "# store the file to submit!\n",
    "test_set_submission_result.to_csv(\"RESULT_to_submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X,y,query_id=column_id_train,zero_based=False,\n",
    "                   f=\"/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/Full_train_forLM.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= [np.random.randint(0, 5) for p in range(0, new_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_id_test=test_change[\"srch_id\"]\n",
    "dump_svmlight_file(new_test,y_test,query_id=column_id_test,zero_based=False,\n",
    "                   f=\"/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/Full_test_forLM.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_loc):\n",
    "    f_train= open(\"/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/Full_train_lm_split.txt\",\"w+\")\n",
    "    f_test= open(\"/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/Full_validation_lm_split.txt\",\"w+\")\n",
    "    f = open(file_loc, 'r')\n",
    "    test_flag=False\n",
    "    i=1\n",
    "    previous_id=0\n",
    "    for line in f:\n",
    "        arr = line.split()\n",
    "        ''' Get the score and query id '''\n",
    "        score = arr[0]\n",
    "        q_id = arr[1].split(':')[1]\n",
    "#         print(q_id)\n",
    "        if previous_id==0:\n",
    "            f_train.write(line)\n",
    "        elif previous_id == q_id:\n",
    "            if test_flag:\n",
    "                f_test.write(line)\n",
    "            else:\n",
    "                f_train.write(line)\n",
    "        else:\n",
    "            if i%9 == 0:\n",
    "                test_flag=True\n",
    "                f_test.write(line)\n",
    "            else:\n",
    "                test_flag=False\n",
    "                f_train.write(line)\n",
    "            i+=1\n",
    "        previous_id=q_id\n",
    "\n",
    "    f.close()\n",
    "    f_train.close()\n",
    "    f_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(\"/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/Full_train_forLM.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_train=open(\"/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/Full_train_forLM.txt\")\n",
    "# full_valid=open(\"/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/Full_validation_lm_split.txt\")\n",
    "# full_test=open('/Users/hernando/Desktop/datamining1/assign2/2nd-assignment-dmt-2020/Full_test_forLM.txt')\n",
    "\n",
    "\n",
    "Train_features, Train_scores, Train_qids, _ = pyltr.data.letor.read_dataset(full_train)\n",
    "# Test_features, Test_scores, Test_qids, _ = pyltr.data.letor.read_dataset(full_test)\n",
    "# Val_features, Val_scores, Val_qids, _ = pyltr.data.letor.read_dataset(full_valid)\n",
    "full_train.close()\n",
    "# full_valid.close()\n",
    "# full_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = pyltr.metrics.NDCG(k=5)\n",
    "\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(Val_features, Val_scores, Val_qids, metric=metric, stop_after=10)\n",
    "\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=metric,\n",
    "    n_estimators=200,\n",
    "    max_features=0.5,\n",
    "    query_subsample=0.5,\n",
    "    max_leaf_nodes=15,\n",
    "    min_samples_leaf=64,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Train_features, Train_scores, Train_qids, monitor=monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random ranking: 0.24553050543774366\n",
      "Our model: 0.2454233979209012\n"
     ]
    }
   ],
   "source": [
    "Epred = model.predict(Test_features)\n",
    "print ('Random ranking:', metric.calc_mean_random(Test_qids, Test_scores))\n",
    "print ('Our model:', metric.calc_mean(Test_qids, Test_scores, Epred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorting the ranking...\n",
      "\n",
      "\n",
      "Store the predictions...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(Epred)\n",
    "submiss_test_set = pd.DataFrame(test_change[\"srch_id\"])\n",
    "submiss_test_set.columns = [\"srch_id\"]\n",
    "submiss_test_set[\"ranking\"] = predictions_df\n",
    "submiss_test_set[\"prop_id\"] = test_change[\"prop_id\"]\n",
    "# Group by search id and sort by ranking!\n",
    "print(\"\\nSorting the ranking...\\n\")\n",
    "test_set_submission_result = submiss_test_set.groupby([\"srch_id\"]).apply(\n",
    "    lambda x: x.sort_values([\"ranking\"], ascending=False)).reset_index(drop=True)\n",
    "print(\"\\nStore the predictions...\\n\")\n",
    "test_set_submission_result = test_set_submission_result.drop(\"ranking\", axis=1)\n",
    "# store the file to submit!\n",
    "test_set_submission_result.to_csv(\"RESULT_to_submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do combine position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
